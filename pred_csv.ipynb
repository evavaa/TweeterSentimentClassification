{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets\n",
    "### Note: This file is used to generate prediction CSV files for data from \"Test.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CSV datafiles (Train and Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 21802\n",
      "Test length: 6099\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "train_data.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n",
    "\n",
    "# separating instance and label for Train and Test\n",
    "X_train_raw = train_data['text']\n",
    "Y_train = train_data['sentiment']\n",
    "X_test_raw = test_data['text']\n",
    "\n",
    "#check the result\n",
    "print(\"Train length:\",len(X_train_raw))\n",
    "print(\"Test length:\",len(X_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import unidecode\n",
    "import contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_digit(word):\n",
    "    '''\n",
    "    Check and return true if a word contains digits.\n",
    "    '''\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Preprocess the raw text data into tokenized lists of words.\n",
    "    Input: a single tweet\n",
    "    Output: a list of filtered terms\n",
    "    '''\n",
    "    # expand contractions (e.g. can't -> cannot)\n",
    "    revised_text = contractions.fix(text)\n",
    "    \n",
    "    # remove links from the text\n",
    "    revised_text = re.sub(r'\\w+:\\/{2}[\\w-]+(\\.[\\w\\/-]+)*', '', revised_text)\n",
    "    \n",
    "    # remove non-ASCII characters\n",
    "    revised_text = re.sub(r'[^\\x00-\\x7F]', r' ', revised_text)\n",
    "\n",
    "    # remove any spacing characters\n",
    "    revised_text = re.sub(r'[\\n\\t\\s]+', r' ', revised_text)\n",
    "    \n",
    "    # tokenize the text into words\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(revised_text)\n",
    "    \n",
    "    # remove stopwords, but keep 'not' and 'no' in text as they indicate negation\n",
    "    keep = ['no', 'not']\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    revised_lst = [w for w in tokens if w in keep or w not in stop_words]\n",
    "    \n",
    "    # remove punctuations in text\n",
    "    revised_lst = [w for w in revised_lst if w not in string.punctuation]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    revised_lst = [w for w in revised_lst if not contain_digit(w)]\n",
    "    \n",
    "    # remove words that are only a single character long\n",
    "    # reduce words back into their stem form except hashtags\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    revised_lst = [w if w[0] == '#' else stemmer.stem(w) for w in revised_lst if len(w) != 1]\n",
    "\n",
    "    return revised_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_tfidf(X_train_raw, X_test_raw, n = 1):\n",
    "    '''\n",
    "    Apply n-gram algorithms while doing TF-IDF vectorization.\n",
    "    n: {1: 'unigram', 2: 'bigram', n: '1-n gram'}, default = 1\n",
    "    '''\n",
    "    if n==1:\n",
    "        # unigram\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer=preprocess)\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "        \n",
    "    else:\n",
    "        # join the tokenized words into sentences\n",
    "        train_x_cleaned = []\n",
    "        test_cleaned = []\n",
    "        \n",
    "        for i in X_train_raw:\n",
    "            train_x_cleaned.append(' '.join(preprocess(i)))\n",
    "        for i in X_test_raw:\n",
    "            test_cleaned.append(' '.join(preprocess(i)))\n",
    "    \n",
    "        if n==2:\n",
    "            # bigram\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "            X_train_tfidf = vectorizer.fit_transform(train_x_cleaned)\n",
    "            X_test_tfidf = vectorizer.transform(test_cleaned)\n",
    "        \n",
    "        else:\n",
    "            # 1-n gram\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1,n))\n",
    "            X_train_tfidf = vectorizer.fit_transform(train_x_cleaned)\n",
    "            X_test_tfidf = vectorizer.transform(test_cleaned)\n",
    "            \n",
    "    return X_train_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(X_train_tfidf, Y_train, sampling_method = None):\n",
    "    '''\n",
    "    Apply sampling method to the cleaned training data.\n",
    "    sampling_method: {'under', 'over', None}, default = None\n",
    "        - 'under': random under sampling\n",
    "        - 'over': random over sampling\n",
    "        - None: no sampling method applied \n",
    "    '''\n",
    "    if sampling_method == 'under':\n",
    "        rus = RandomUnderSampler(random_state=42) \n",
    "        return rus.fit_resample(X_train_tfidf, Y_train)\n",
    "    elif sampling_method == 'over':\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        return ros.fit_resample(X_train_tfidf, Y_train)\n",
    "    else:\n",
    "        return X_train_tfidf, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kBest_chi2(i: int, X_train_smp, Y_train, X_test_tfidf):\n",
    "    '''\n",
    "    Select the first i best features using Chi-square test. \n",
    "    '''\n",
    "    x2 = SelectKBest(chi2, k=i)\n",
    "    X_train_kBest = x2.fit_transform(X_train_smp,Y_train)\n",
    "    X_test_kBest = x2.transform(X_test_tfidf)\n",
    "    return X_train_kBest, X_test_kBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space size (using TFIDF): (21802, 22333)\n",
      "Test feature space size (using TFIDF): (6099, 22333)\n",
      "\n",
      "\n",
      "Train feature space size (after feature selection): (11145, 2233)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply tfidf vectorization\n",
    "X_train_tfidf, X_test_tfidf = n_gram_tfidf(X_train_raw, X_test_raw, 1)\n",
    "feature_size = X_train_tfidf.shape[1]\n",
    "print(\"Train feature space size (using TFIDF):\",X_train_tfidf.shape)\n",
    "print(\"Test feature space size (using TFIDF):\",X_test_tfidf.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "# sampling \n",
    "X_train_smp, Y_train_smp = sampling(X_train_tfidf, Y_train, sampling_method = 'under') # adjust sampling methods here\n",
    "\n",
    "# choose k best (top 10%) features using chi2 test\n",
    "X_train_kBest, X_test_kBest = kBest_chi2(int(0.1*feature_size), X_train_smp, Y_train_smp, X_test_tfidf)\n",
    "print(\"Train feature space size (after feature selection):\", X_train_kBest.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = {\n",
    "    ('lg', LogisticRegression(solver='saga', multi_class='multinomial', C=6, max_iter=1000, penalty = 'l2')),\n",
    "    ('svm', SVC(kernel='rbf', C=5)),\n",
    "    ('bnb', BernoulliNB()),\n",
    "}\n",
    "\n",
    "stk_clf = StackingClassifier(estimators, final_estimator=LogisticRegression(solver='saga', \n",
    "                            multi_class='multinomial', C=6, max_iter=10000, penalty = 'l2'))\n",
    "\n",
    "stk_clf.fit(X_train_kBest, Y_train_smp)\n",
    "prediction = stk_clf.predict(X_test_kBest)\n",
    "result = {'id': test_data['id'].tolist(), 'sentiment': prediction}\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.to_csv('prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
