{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets\n",
    "### NOTE: This file is used to apply 10-fold cross validation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CSV datafiles (Train and Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 21802\n",
      "Test length: 6099\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "train_data.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n",
    "\n",
    "# separating instance and label for Train and Test\n",
    "X_train_raw = train_data['text']\n",
    "Y_train = train_data['sentiment']\n",
    "X_test_raw = test_data['text']\n",
    "\n",
    "#check the result\n",
    "print(\"Train length:\",len(X_train_raw))\n",
    "print(\"Test length:\",len(X_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import unidecode\n",
    "import contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_digit(word):\n",
    "    '''\n",
    "    Check and return true if a word contains digits.\n",
    "    '''\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Preprocess the raw text data into tokenized lists of words.\n",
    "    Input: a single tweet\n",
    "    Output: a list of filtered terms\n",
    "    '''\n",
    "    # expand contractions (e.g. can't -> cannot)\n",
    "    revised_text = contractions.fix(text)\n",
    "    \n",
    "    # remove links from the text\n",
    "    revised_text = re.sub(r'\\w+:\\/{2}[\\w-]+(\\.[\\w\\/-]+)*', '', revised_text)\n",
    "    \n",
    "    # remove non-ASCII characters\n",
    "    revised_text = re.sub(r'[^\\x00-\\x7F]', r' ', revised_text)\n",
    "\n",
    "    # remove any spacing characters\n",
    "    revised_text = re.sub(r'[\\n\\t\\s]+', r' ', revised_text)\n",
    "    \n",
    "    # tokenize the text into words\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(revised_text)\n",
    "    \n",
    "    # remove stopwords, but keep 'not' and 'no' in text as they indicate negation\n",
    "    keep = ['no', 'not']\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    revised_lst = [w for w in tokens if w in keep or w not in stop_words]\n",
    "    \n",
    "    # remove punctuations in text\n",
    "    revised_lst = [w for w in revised_lst if w not in string.punctuation]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    revised_lst = [w for w in revised_lst if not contain_digit(w)]\n",
    "    \n",
    "    # remove words that are only a single character long\n",
    "    # reduce words back into their stem form except hashtags\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    revised_lst = [w if w[0] == '#' else stemmer.stem(w) for w in revised_lst if len(w) != 1]\n",
    "\n",
    "    return revised_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_tfidf(X_train_raw, X_test_raw, n = 1):\n",
    "    '''\n",
    "    Apply n-gram algorithms while doing TF-IDF vectorization.\n",
    "    n: {1: 'unigram', 2: 'bigram', n: '1-n gram'}, default = 1\n",
    "    '''\n",
    "    if n==1:\n",
    "        # unigram\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer=preprocess)\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "        \n",
    "    else:\n",
    "        # join the tokenized words into sentences\n",
    "        train_x_cleaned = []\n",
    "        test_cleaned = []\n",
    "        \n",
    "        for i in X_train_raw:\n",
    "            train_x_cleaned.append(' '.join(preprocess(i)))\n",
    "        for i in X_test_raw:\n",
    "            test_cleaned.append(' '.join(preprocess(i)))\n",
    "    \n",
    "        if n==2:\n",
    "            # bigram\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "            X_train_tfidf = vectorizer.fit_transform(train_x_cleaned)\n",
    "            X_test_tfidf = vectorizer.transform(test_cleaned)\n",
    "        \n",
    "        else:\n",
    "            # 1-n gram\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1,n))\n",
    "            X_train_tfidf = vectorizer.fit_transform(train_x_cleaned)\n",
    "            X_test_tfidf = vectorizer.transform(test_cleaned)\n",
    "            \n",
    "    return X_train_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(X_train_tfidf, Y_train, sampling_method = None):\n",
    "    '''\n",
    "    Apply sampling method to the cleaned training data.\n",
    "    sampling_method: {'under', 'over', None}, default = None\n",
    "        - 'under': random under sampling\n",
    "        - 'over': random over sampling\n",
    "        - None: no sampling method applied \n",
    "    '''\n",
    "    if sampling_method == 'under':\n",
    "        rus = RandomUnderSampler(random_state=42) \n",
    "        return rus.fit_resample(X_train_tfidf, Y_train)\n",
    "    elif sampling_method == 'over':\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        return ros.fit_resample(X_train_tfidf, Y_train)\n",
    "    else:\n",
    "        return X_train_tfidf, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kBest_chi2(i: int, X_train_smp, Y_train, X_test_tfidf):\n",
    "    '''\n",
    "    Select the first i best features using Chi-square test. \n",
    "    '''\n",
    "    x2 = SelectKBest(chi2, k=i)\n",
    "    X_train_kBest = x2.fit_transform(X_train_smp,Y_train)\n",
    "    X_test_kBest = x2.transform(X_test_tfidf)\n",
    "    return X_train_kBest, X_test_kBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_r = DummyClassifier(strategy=\"most_frequent\")\n",
    "bnb = BernoulliNB()\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "svm = SVC(kernel='rbf', C=5, decision_function_shape='ovo')\n",
    "lg_clf = LogisticRegression(solver='saga', multi_class='multinomial', C=6, \n",
    "                            class_weight='balanced', max_iter=1000, penalty = 'l2')\n",
    "estimators = {\n",
    "    ('lg', LogisticRegression(solver='saga', multi_class='multinomial', C=6, max_iter=1000, penalty = 'l2')),\n",
    "    ('svm', SVC(kernel='rbf', C=5)),\n",
    "    ('bnb', BernoulliNB()),\n",
    "}\n",
    "\n",
    "stk_clf = StackingClassifier(estimators, final_estimator=LogisticRegression(solver='saga', \n",
    "                            multi_class='multinomial', C=6, max_iter=10000, penalty = 'l2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "def cross_validation(X_train_raw, Y_train, fold=10):\n",
    "    total_acc = {\"BNB\":[0]*10, \"KNN\":[0]*10, \"LG\":[0]*10, \"SVM\":[0]*10, \"STK\":[0]*10}\n",
    "    total_recall = {\"BNB\":[0]*10, \"KNN\":[0]*10, \"LG\":[0]*10, \"SVM\":[0]*10, \"STK\":[0]*10}\n",
    "    total_precision = {\"BNB\":[0]*10, \"KNN\":[0]*10, \"LG\":[0]*10, \"SVM\":[0]*10, \"STK\":[0]*10}\n",
    "    total_f1 = {\"BNB\":[0]*10, \"KNN\":[0]*10, \"LG\":[0]*10, \"SVM\":[0]*10, \"STK\":[0]*10}\n",
    "    \n",
    "    # iteratively divide data into train/test set\n",
    "    num_test_data = int(X_train_raw.shape[0]/fold)\n",
    "    for i in range(fold):\n",
    "        X_val = X_train_raw[i*num_test_data:(i+1)*num_test_data]\n",
    "        X_train = pd.concat([X_train_raw[:i*num_test_data], X_train_raw[(i+1)*num_test_data:]], axis=0)\n",
    "        y_val = Y_train[i*num_test_data:(i+1)*num_test_data]\n",
    "        y_train = pd.concat([Y_train[:i*num_test_data], Y_train[(i+1)*num_test_data:]], axis=0)\n",
    "      \n",
    "        X_train_tfidf, X_val_tfidf = n_gram_tfidf(X_train, X_val, n=1)  # adjust n-gram approach here\n",
    "        feature_size = X_train_tfidf.shape[1]\n",
    "        \n",
    "        # sampling \n",
    "        X_train_smp, Y_train_smp = sampling(X_train_tfidf, y_train, sampling_method = 'under') # adjust sampling methods here\n",
    "        # sub-sample the validation set to obtain a balanced dataset\n",
    "        X_val_tfidf, y_val = sampling(X_val_tfidf, y_val, sampling_method = 'under')\n",
    "\n",
    "        # choose k best (top 10%) features using chi2 test\n",
    "        X_train_kBest, X_val_kBest = kBest_chi2(int(0.1*feature_size), X_train_smp, Y_train_smp, X_val_tfidf)\n",
    "        \n",
    "        clf_dict = {bnb: \"BNB\", knn5: \"KNN\", lg_clf: \"LG\", svm: \"SVM\", stk_clf: \"STK\"}\n",
    "        for clf in clf_dict:\n",
    "            acc = []\n",
    "            X_train_kBest, X_val_kBest = kBest_chi2(int(0.1*feature_size), X_train_smp, Y_train_smp, X_val_tfidf)\n",
    "            \n",
    "            clf.fit(X_train_kBest, Y_train_smp)\n",
    "            prediction = clf.predict(X_val_kBest)\n",
    "\n",
    "            total_acc[clf_dict[clf]][i] = accuracy_score(y_val, prediction)\n",
    "            total_f1[clf_dict[clf]][i] = f1_score(y_val, prediction, average='macro')\n",
    "            total_precision[clf_dict[clf]][i] = precision_score(y_val, prediction, average='macro')\n",
    "            total_recall[clf_dict[clf]][i] = recall_score(y_val, prediction, average='macro')\n",
    "            \n",
    "            \n",
    "    for clf in clf_dict:\n",
    "        print(clf_dict[clf])\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        print(\"The average accuracy is: \", sum(total_acc[clf_dict[clf]])/fold)\n",
    "        print(\"The average recall is: \", sum(total_recall[clf_dict[clf]])/fold)\n",
    "        print(\"The average precision is: \", sum(total_precision[clf_dict[clf]])/fold)\n",
    "        print(\"The average f1 is: \", sum(total_f1[clf_dict[clf]])/fold)\n",
    "        print(\"\\n\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB\n",
      "-------------------------------------------------------\n",
      "The average accuracy is:  0.6251553523262963\n",
      "The average recall is:  0.6251553523262963\n",
      "The average precision is:  0.619061009525415\n",
      "The average f1 is:  0.6197547473181317\n",
      "\n",
      "\n",
      "KNN\n",
      "-------------------------------------------------------\n",
      "The average accuracy is:  0.49629535197386854\n",
      "The average recall is:  0.49629535197386854\n",
      "The average precision is:  0.5401614117000091\n",
      "The average f1 is:  0.49746491135229975\n",
      "\n",
      "\n",
      "LG\n",
      "-------------------------------------------------------\n",
      "The average accuracy is:  0.6300765624417339\n",
      "The average recall is:  0.630076562441734\n",
      "The average precision is:  0.6297998011282645\n",
      "The average f1 is:  0.6297496042769579\n",
      "\n",
      "\n",
      "SVM\n",
      "-------------------------------------------------------\n",
      "The average accuracy is:  0.6217596364266736\n",
      "The average recall is:  0.6217596364266736\n",
      "The average precision is:  0.621470883561944\n",
      "The average f1 is:  0.6212804913256159\n",
      "\n",
      "\n",
      "STK\n",
      "-------------------------------------------------------\n",
      "The average accuracy is:  0.6397464630770932\n",
      "The average recall is:  0.6397464630770932\n",
      "The average precision is:  0.6371751366930413\n",
      "The average f1 is:  0.638081011878435\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validation(X_train_raw, Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
